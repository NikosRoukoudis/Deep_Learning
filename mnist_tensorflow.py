# -*- coding: utf-8 -*-
"""MNIST - Tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YG2XeDsld2w30WLYoROemUNZck5bZ5pg

# Import Libraries
"""

import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

"""# Setting Environment Seeds

Seeds enable the reproduction of the experiments.
"""

random.seed(0)
np.random.seed(seed=0)
tf.random.set_seed(seed=0)

"""# Load MNIST Dataset using Tensorflow"""

(x_train, y_train), (x_test, y_test ) = tf.keras.datasets.mnist.load_data()
x_train.shape, y_train.shape, x_test.shape, y_test.shape

print(
    f'Training Size - Inputs:{x_train.shape}, Targets:{y_train.shape}'
    f'\nTest Size - Inputs:{x_test.shape}, Targets:{y_test.shape}'
)

"""# Displaying Random Digits"""

rows = 5
digits_per_row = 5

fig, axes = plt.subplots(nrows=rows, ncols=digits_per_row, figsize=(6, 6))
axes = axes.flatten()

# Selecting random ids from 0 to 60000
total_digits = rows*digits_per_row
random_ids= np.random.choice(x_train.shape[0], total_digits, replace=False)

# Plotting the selected digits.
for i, ax in enumerate(axes):
    idx = random_ids[i]
    ax.imshow(x_train[idx], cmap='gray')
    ax.set_title(f'Class: {y_train[idx]}')
    ax.axis('off')
plt.tight_layout()
plt.show()

"""# Preprocessing Inputs


*   Deep Neural Networks require vectors are inputs. Since the digits are 28x28
*   One-hot Encoding is required for targets, since we use 10 classes. (e.g. 0 will be classified as 0 0 0 0 0 0 0 0 0 1, one will be classified as 0 0 0 0 0 0 0 0 1 0, etc.

grayscale images, they should be converted to vectors of 784 size.
"""

x_train = x_train.reshape((60000, 784))
y_train = tf.one_hot(y_train, depth=10)
x_test = x_test.reshape((10000, 784))
y_test = tf.one_hot(y_test, depth=10)

print(
    f'Training Size - Inputs:{x_train.shape}, Targets:{y_train.shape}'
    f'\nTest Size - Inputs:{x_test.shape}, Targets:{y_test.shape}'
)

"""# Construct Deep Neural Network





*   Input: 784 Features
*   Hiddens: 2 Hiddens of 256 units, which pass through tanh activation.
*   Output: 10 probabilities (1 for each class). Softmax activation is required to convert the network's outputs into probabilities.


"""

activation = 'relu'
loss = 'categorical_crossentropy' # Do not change this loss function.
metrics = ['accuracy']
learning_rate = 0.0005
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate) # Do not change this optimizer.
epochs = 15

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(784,), name='input'),
    tf.keras.layers.Dense(units=512, activation=activation, name='hidden-1'),
    tf.keras.layers.Dense(units=256, activation=activation, name='hidden-2'),
    tf.keras.layers.Dense(units=128, activation=activation, name='hidden-3'),
    tf.keras.layers.Dense(units=10, activation='softmax', name='outputs') # Do not change this activation function.
])
model.summary(expand_nested=True)

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
history = model.fit(
    x=x_train,
    y=y_train,
    epochs=epochs,
    validation_data=(x_test, y_test)
)

"""# Display Loss and Accuracy per Training Epoch"""

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Neural Network Loss per epoch')
plt.ylabel('Categorical Cross-Entropy')
plt.xlabel('Epochs')
plt.xlim(0, epochs)
plt.ylim(0, 1)
plt.legend()
plt.show()

plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Neural Network Accuracy per epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.xlim(0, epochs)
plt.ylim(0, 1)
plt.legend()
plt.show()

"""# --- TODOs ---

## 1. Hyperparameter Tuning: Try adjusting number of hidden layers, hidden units, activation function, learning rate, number of epochs, etc.

| Hidden Layers    | Dropout | LR     | Activation | Epochs | Accuracy (Val) |
| ---------------- | ------- | ------ | ---------- | ------ | -------------- |
| \[128, 64]       | 0.2     | 0.001  | relu       | 10     | \~0.98         |
| \[256, 128]      | 0.3     | 0.001  | relu       | 10     | \~0.984        |
| \[512, 256, 128] | 0.4     | 0.0005 | relu       | 15     | \~0.986        |

## 2. Validation: Show 1 misclassified digit from each class.
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1)
datagen.fit(x_train.reshape(-1, 28, 28, 1))

model_aug = build_model([512, 256, 128], 'relu', 0.4, 0.0005)
history_aug = model_aug.fit(datagen.flow(x_train, y_train_cat, batch_size=128),
                            epochs=15, validation_data=(x_test, y_test_cat))

"""## 3. Suggest & Implement workflow improvements, aside hyperparameter tuning. Can you think of any ways to increase the overall accuracy of the Neural Network?"""

print("""
1a. Το MNIST είναι καλό dataset για εισαγωγή στη βαθιά μάθηση επειδή είναι καθαρό, ισορροπημένο και έχει επαρκές μέγεθος.

1b. Όχι όλα τα pixels δεν είναι το ίδιο σημαντικά. Πολλά περιφερειακά pixels είναι πάντα μαύρα (0) και δεν συνεισφέρουν στην πρόβλεψη.

1c. Τα Βαθιά Νευρωνικά Δίκτυα είναι κατάλληλα όταν το πρόβλημα έχει μεγάλη πολυπλοκότητα, υψηλής διάστασης δεδομένα, και απαιτεί εξαγωγή μη γραμμικών σχέσεων.

1d. Ναι, η Βαθιά Μάθηση μπορεί να εφαρμοστεί και στους τρεις τομείς της ML:
 - Supervised (πχ ταξινόμηση εικόνων),
 - Unsupervised (πχ autoencoders, clustering με embeddings),
 - Reinforcement Learning (πχ DQN για παιχνίδια ή ρομποτική).
""")

"""## 4. Upload the exercise in your GitHub repository. Google Colab can instantly deploy this notebook into a Github repository."""



"""## 5. Write a README file in your github repository, explaining:

1. Your workflow
2. The final model architecture
3. The selected hyperparameters
4. Include the requirements.txt file, which shows the python version, as well as the library versions.
"""

!pip show tensorflow